---
layout: post
title: "From Nerual Network to Deep learning - 从神经网络到深度学习"
date:   2017-10-01 10:06:46 +0800
categories: machine learning
published: false
---

# Background
#### 灵感来源
- 人脑有860亿个神经元
- 神经元切换时间> 10-3秒
- 脸部识别〜0.1秒
- 平均每个神经元有几千个连接
- 每秒数百个操作
- 高度并行计算
- 分布式表示

#### 从Biological Neuron生物神经系统 到 Artificial Neuron人造神经系统
<img src="{{ site.baseurl }}/img/machinelearning/neuron01.png" alt="neuron01" style="width: 500px;"/>  
- Many simple neuron - like threshold switching units
- Many weighted interconnections among units
- Highly parallel, distributed processing
- Learning by tuning the connection weights   
<img src="{{ site.baseurl }}/img/machinelearning/neuron02.png" alt="neuron02" style="width: 200px;"/>  

# Nerual Network
1. Perceptron: Linear Threshold Unit (1950’s)

2. Perceptrons for Logical AND

3. Gradient Descent Learning

4. Multi-layer Perceptron Network (1960-80’s)

5. Back Propagation

# Problems of Nerual Network
- Requires labeled data     
    » Most data is unlabeled
- Backpropagation solutions may be trapped in local minima
- Large networks (> 2 hidden layers) are harder to train 
    - Vanishing gradients
- Overfitting becomes a serious issue
- 反向传播算法 (back-prop) 在生物学上是很难成立的，很难相信神经系统能够自动形成与正向传播对应的反向传播结构

所以直到Deep Learning出来之前，SVM是老大。

# Until 2006 - Breakthrough work by GE Hinton’s group
- Proposed simple neural nets called **Restricted Boltzmann Machines (RBM)**, and stacked them to develop Deep Belief Networks (DBN’s)
- Came up with a **fast algorithm** for training them
- Demonstrated DBN beats state-of-the-art significantly

