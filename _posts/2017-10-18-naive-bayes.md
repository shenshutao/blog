---
layout: splash
title: "Naive Bayes Algorithm - 朴素贝叶斯分类算法"
date:   2017-10-18 10:06:46 +0800
categories:
- machinelearning
tags:
- machinelearning
- bayesian
comments: true
share: true
publish: true
---
<style type="text/css">
    table {
        width: 30%;
    }
</style>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
# Naive Bayes 分类算法 （多项式分布）

## 简介 
Naive Bayes 分类是基于贝叶斯定理和特征独立性假设的分类算法。     
本文主要讲(参数)多项分布下的贝叶斯定理。    

## 推导 1
#### 由条件概率公式
$$ P(A|B) = P(AB) / P(B) $$    
$$ P(B|A) = P(AB) / P(A)$$  

#### 得以下两个乘法公式：    
$$ P(AB) = P(B) * P(A|B) $$    
$$ P(AB) = P(A) * P(B|A) $$     
   
#### 然后可以得出：     
$$ P(B) * P(A|B) = P(A) * P(B|A) $$   
即：   
$$ P(A|B) = \frac{P(B|A) * P(A)} {P(B)} $$     
我们可以看出来，贝叶斯定理是描述P(A|B)和P(B|A)之间的关系的一条定理。   
可以用贝叶斯定理由P(A|B)，结合P(A)和P(B)推导出P(B|A)。   

## 推导 2
#### 由推导1 可以得出： 
$$ P(Y|X_{1}X_{2}...X_{n}) = \frac{P(Y) * P(X_{1}X_{2}...X_{n}|Y)} {P(X_{1}X_{2}...X_{n})} $$      

#### 引入条件独立假设：
为了方便处理 $$ P(X_{1}X_{2}...X_{n}|Y) $$ ，我们假设在Y条件下，所有$$ X_{1},X_{2},...,X_{n} $$独立，基于这个条件独立假设，我们得到等式：     
$$ P(X_{1}X_{2}...X_{n}|Y) = P(X_{1}|Y)P(X_{2}|Y)...P(X_{n}|Y)  即 \prod_{i=1}^{n}P(X_{i}|Y) $$ 

#### 结合前两者，我们得到等式
$$ P(Y|X_{1}X_{2}...X_{n}) = \frac{P(Y) * \prod_{i=1}^{n}P(X_{i}|Y)} {P(X_{1}X_{2}...X_{n})} $$ 

#### 当然，在我们进行分类的时候,分母的值不重要：
- 在$$ X_{1}X_{2}...X_{n} $$情况下，分类是yes的概率为：
$$ P(Y|X_{1}X_{2}...X_{n}) = \frac{P(Y) * \prod_{i=1}^{n}P(X_{i}|Y)} {P(X_{1}X_{2}...X_{n})} $$ 
- 在$$ X_{1}X_{2}...X_{n} $$情况下，结果是no的概率为：
$$ P(N|X_{1}X_{2}...X_{n}) = \frac{P(N) * \prod_{i=1}^{n}P(X_{i}|N)} {P(X_{1}X_{2}...X_{n})} $$ 

分类即是比较概率大小，既然大家的分母$$ {P(X_{1}X_{2}...X_{n})} $$ 都是一样的，我们可以将它拿掉。

#### 得到贝叶斯算法的多项式公式: 
$$ \underset{c}{argmax}  P(C=c)*\prod_{i=1}^{n}P(F_{i}=f_{i}|C=c) $$

## 应用举例
#### 已有数据集 (训练集)：

畏寒 | 流鼻涕 | 头痛 | 发烧 | 流感    
---- | --- | --- | --- | ---     
Y | N | 中等 | Y | N  
Y | Y | 无 | N | Y  
Y | N | 强烈 | Y | Y  
N | Y | 中等 | Y | Y  
N | N | 无 | N | N     
N | Y | 强烈 | Y | Y  
N | Y | 强烈 | N | N  
Y | Y | 中等 | Y | Y  

#### 判断是否为流感(测试)：

畏寒 | 流鼻涕 | 头痛 | 发烧 | 流感    
---- | --- | --- | --- | ---     
Y | N | 中等 | N | ???

#### 步骤：
1. 计算P(流感=Y) 和 P(流感=N)
```
P(流感=Y) = 5 / 8 = 0.625, P(流感=N) = 3 / 8 = 0.375
```
2. 把流感=Y和流感=N的案例分开来, 分别计算各种参数在流感=Y和流感=N的情况下的各种概率：
```
P(畏寒=Y|流感=Y) = 3 / 5 = 0.6， P(畏寒=Y|流感=N) = 1 / 3 = 0.333
P(畏寒=N|流感=Y) = 2 / 5 = 0.4， P(畏寒=N|流感=N) = 2 / 3 = 0.666
P(流鼻涕=Y|流感=Y) = 4 / 5 = 0.8， P(流鼻涕=Y|流感=N) = 1 / 3 = 0.333
P(流鼻涕=N|流感=Y) = 1 / 5 = 0.2， P(流鼻涕=N|流感=N) = 2 / 3 = 0.666
P(头痛=强烈|流感=Y) = 2 / 5 = 0.4， P(头痛=强烈|流感=N) = 1 / 3 = 0.333
P(头痛=中等|流感=Y) = 2 / 5 = 0.4， P(头痛=中等|流感=N) = 1 / 3 = 0.333
P(头痛=无|流感=Y) = 1 / 5 = 0.2， P(头痛=无|流感=N) = 1 / 3 = 0.333
P(发烧=Y|流感=Y) = 4 / 5 = 0.8， P(发烧=Y|流感=N) = 1 / 3 = 0.333
P(发烧=N|流感=Y) = 1 / 5 = 0.2， P(发烧=N|流感=N) = 2 / 3 = 0.666
```

3. 分别计算测试情况下，流感为Y 和 流感为N 的概率，即：  
```  
P(流感=Y|畏寒=Y，流鼻涕=N，头痛=中等，发烧=N)     
= P(流感=Y)·P(畏寒=Y|流感=Y)·P(流鼻涕=N|流感=Y)·P(头痛=中等|流感=Y)·P(发烧=N|流感=Y)   
= 0.625 \* 0.6 \* 0.2 \* 0.4 \* 0.2     
= 0.006     
P(流感=N|畏寒=Y，流鼻涕=N，头痛=中等，发烧=N)    
= P(流感=N)·P(畏寒=Y|流感=N)·P(流鼻涕=N|流感=N)·P(头痛=中等|流感=N)·P(发烧=N|流感=N)
= 0.375 \* 0.333 \* 0.666 \* 0.333 \* 0.666    
= 0.185
```

4. 标准化(可选)   
我们注意到上面两个概率相加并不等于1，    
虽然不影响分类，但我们可以把他们标准化：    
P(流感=Y|畏寒=Y，流鼻涕=N，头痛=中等，发烧=N) = $$ \frac{0.006}{0.006+0.185} = 0.031 $$    
P(流感=N|畏寒=Y，流鼻涕=N，头痛=中等，发烧=N) = $$ \frac{0.185}{0.006+0.185} = 0.969 $$

5. 结论   
此测试记录不是流感的概率比较高，为0.969，分类为 流感=N。

## 优点
- 算法比较简单
- 发源于古典数学，分类效率稳定
- 对小规模数据表现良好
- 在数据量较大时，可以对模型进行增量式训练，减小对内存的要求
- 对缺失数据不敏感

## 缺点
- 现实中，条件独立性假设很难完全实现。
- 多项式的贝叶斯分类模型只适用于离散型的参数，对于连续型的数值需先进行分级(binning)。


